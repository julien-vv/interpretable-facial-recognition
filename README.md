# Interpretable Facial Recognition

> Research and development project focused on improving the interpretability of deep learning models applied to facial recognition tasks.

## About

This project explores techniques to make Convolutional Neural Networks (CNNs) more interpretable when applied to facial recognition.  
The main focus areas include:

- Filter visualization
- Grad-CAM (Gradient-weighted Class Activation Mapping) analysis
- Mask generation and photo anonymization
- Analysis of algorithmic bias

The goal is to better understand how deep learning models make decisions, particularly in sensitive applications such as facial recognition.

